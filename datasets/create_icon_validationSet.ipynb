{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import collections\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def bfs(map_visit, map_connect, ii):\n",
    "    queue = collections.deque()\n",
    "    queue.append((map_connect[ii], ii))\n",
    "\n",
    "    union = []\n",
    "    if not map_visit[ii]:\n",
    "        union.append(ii)\n",
    "        map_visit[ii] = True\n",
    "    while queue:\n",
    "        connects, ii = queue.popleft()\n",
    "        for connect in connects:\n",
    "            if not map_visit[connect]:\n",
    "                queue.append((map_connect[connect], connect))\n",
    "                map_visit[connect] = True\n",
    "                union.append(connect)\n",
    "    return union\n",
    "\n",
    "def plot_save(image, filename):\n",
    "    fig = plt.figure(dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image, aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "def get_paths(lfw_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    # add new rule, pair = 5\n",
    "    for pair in pairs:\n",
    "        if len(pair) == 3:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[2]))\n",
    "            issame = True\n",
    "        elif len(pair) == 4:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[3]))\n",
    "            issame = False\n",
    "        elif len(pair) == 5:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[3]))\n",
    "            if pair[4] == 'T':\n",
    "                issame = True\n",
    "            elif pair[4] == 'F':\n",
    "                issame = False\n",
    "        if os.path.exists(path0) and os.path.exists(path1):  # Only add the pair if both paths exist\n",
    "            path_list += (path0, path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs > 0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "\n",
    "    return path_list, issame_list\n",
    "\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path+'.jpg'):\n",
    "        return path+'.jpg'\n",
    "    elif os.path.exists(path+'.png'):\n",
    "        return path+'.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)\n",
    "\n",
    "def read_pairs(pairs_filename):\n",
    "    pairs = []\n",
    "    with open(pairs_filename, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "    return np.array(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1449011786' '0' '1095451819' '0' 'T']\n",
      " ['1448039576' '0' '1185423527' '0' 'F']\n",
      " ['1447550782' '0' '1462095693' '0' 'T']\n",
      " ['1458379276' '0' '1381475296' '0' 'F']\n",
      " ['1447550782' '0' '1230636915' '0' 'T']\n",
      " ['1447059340' '0' '1462029352' '0' 'F']]\n",
      "['/root/icon180k_image_160/1449011786/0.png', '/root/icon180k_image_160/1095451819/0.png', '/root/icon180k_image_160/1448039576/0.png', '/root/icon180k_image_160/1185423527/0.png', '/root/icon180k_image_160/1447550782/0.png', '/root/icon180k_image_160/1462095693/0.png', '/root/icon180k_image_160/1458379276/0.png', '/root/icon180k_image_160/1381475296/0.png']\n"
     ]
    }
   ],
   "source": [
    "lfw_pairs='/root/facenet_davidsandberg/data/icon_valv2.txt'\n",
    "lfw_dir='/root/icon180k_image_160/'\n",
    "\n",
    "pairs = read_pairs(os.path.expanduser(lfw_pairs))\n",
    "print(pairs[0:6])\n",
    "# Get the paths for the corresponding images\n",
    "lfw_paths, actual_issame = get_paths(os.path.expanduser(lfw_dir), pairs)\n",
    "print(lfw_paths[0:8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create icon validation for npair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 12 17:34:25 2019\n",
    "\n",
    "@author: Cheng-Hung\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import fuel\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def preprocess(hwc_bgr_image, size):\n",
    "    hwc_rgb_image = cv2.cvtColor(hwc_bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    resized = cv2.resize(hwc_rgb_image, (size))\n",
    "    chw_image = np.transpose(resized, axes=(2, 0, 1))\n",
    "    return chw_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dataset_name = \"cars196\"\n",
    "    archive_basename = \"car_ims\"\n",
    "\n",
    "    fuel_root_path = \"./data\"\n",
    "#     fuel_root_path = \"./datasets/data\"\n",
    "    fuel_data_path = os.path.join(fuel_root_path, dataset_name)\n",
    "#     image_filepath = os.path.join(fuel_data_path, archive_basename + \".tar.gz\")\n",
    "#     label_filepath = os.path.join(fuel_data_path, \"cars_annos.mat\")\n",
    "\n",
    "#     # Extract car_ims.tgz if car_ims directory does not exist\n",
    "#     with tarfile.open(image_filepath, \"r\") as tf:\n",
    "#         jpg_filenames = [fn for fn in tf.getnames() if fn.endswith(\".jpg\")]\n",
    "#     jpg_filenames.sort()\n",
    "#     num_examples = len(jpg_filenames)  # ????\n",
    "#     if not os.path.exists(os.path.join(fuel_data_path, archive_basename)):\n",
    "#         subprocess.call([\"tar\", \"zxvf\", image_filepath.replace(\"\\\\\", \"/\"),\n",
    "#                          \"-C\", fuel_data_path.replace(\"\\\\\", \"/\"),\n",
    "#                          \"--force-local\"])\n",
    "jpg_filenames = lfw_paths\n",
    "num_examples = len(jpg_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "    # Extract class labels\n",
    "#     cars_annos = loadmat(label_filepath)\n",
    "#     annotations = cars_annos[\"annotations\"].ravel()\n",
    "#     annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n",
    "    annotations = range(num_examples)\n",
    "    class_labels = []\n",
    "    for annotation in annotations:\n",
    "#         class_label = int(annotation[5])\n",
    "        class_labels.append(annotation)\n",
    "\n",
    "print(class_labels[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # open hdf5 file\n",
    "    hdf5_filename = \"cars196.hdf5\"\n",
    "    hdf5_filepath = os.path.join(fuel_data_path, hdf5_filename)\n",
    "    hdf5 = h5py.File(hdf5_filepath, mode=\"w\")\n",
    "\n",
    "    # store images\n",
    "    image_size = (256, 256)\n",
    "    array_shape = (num_examples, 3) + image_size\n",
    "    ds_images = hdf5.create_dataset(\"images\", array_shape, dtype=np.uint8)\n",
    "    ds_images.dims[0].label = \"batch\"\n",
    "    ds_images.dims[1].label = \"channel\"\n",
    "    ds_images.dims[2].label = \"height\"\n",
    "    ds_images.dims[3].label = \"width\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./data/cars196/cars196.hdf5: 100%|██████████| 6000/6000 [00:08<00:00, 688.01it/s]\n"
     ]
    }
   ],
   "source": [
    "    # write images to the disk\n",
    "    for i, filename in tqdm(enumerate(jpg_filenames), total=num_examples,\n",
    "                            desc=hdf5_filepath):\n",
    "        raw_image = cv2.imread( filename,\n",
    "                               cv2.IMREAD_COLOR)  # BGR image\n",
    "        image = preprocess(raw_image, image_size)\n",
    "        ds_images[i] = image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # store the targets (class labels)\n",
    "    targets = np.array(class_labels, np.int32).reshape(num_examples, 1)\n",
    "    ds_targets = hdf5.create_dataset(\"targets\", data=targets)\n",
    "    ds_targets.dims[0].label = \"batch\"\n",
    "    ds_targets.dims[1].label = \"class_labels\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "    # specify the splits (labels 1~98 for train, 99~196 for test)\n",
    "    test_head = class_labels.index(0)\n",
    "    print(test_head)\n",
    "    split_train, split_test = (0, test_head), (test_head, num_examples)\n",
    "    split_dict = dict(train=dict(images=split_train, targets=split_train),\n",
    "                      test=dict(images=split_test, targets=split_test))\n",
    "    hdf5.attrs[\"split\"] = H5PYDataset.create_split_array(split_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hdf5.flush()\n",
    "    hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
