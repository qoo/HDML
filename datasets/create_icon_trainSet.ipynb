{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import collections\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def bfs(map_visit, map_connect, ii):\n",
    "    queue = collections.deque()\n",
    "    queue.append((map_connect[ii], ii))\n",
    "\n",
    "    union = []\n",
    "    if not map_visit[ii]:\n",
    "        union.append(ii)\n",
    "        map_visit[ii] = True\n",
    "    while queue:\n",
    "        connects, ii = queue.popleft()\n",
    "        for connect in connects:\n",
    "            if not map_visit[connect]:\n",
    "                queue.append((map_connect[connect], connect))\n",
    "                map_visit[connect] = True\n",
    "                union.append(connect)\n",
    "    return union\n",
    "\n",
    "def plot_save(image, filename):\n",
    "    fig = plt.figure(dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image, aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "def get_paths(lfw_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    # add new rule, pair = 5\n",
    "    for pair in pairs:\n",
    "        if len(pair) == 3:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[2]))\n",
    "            issame = True\n",
    "        elif len(pair) == 4:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[3]))\n",
    "            issame = False\n",
    "        elif len(pair) == 5:\n",
    "            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[3]))\n",
    "            if pair[4] == 'T':\n",
    "                issame = True\n",
    "            elif pair[4] == 'F':\n",
    "                issame = False\n",
    "        if os.path.exists(path0) and os.path.exists(path1):  # Only add the pair if both paths exist\n",
    "            path_list += (path0, path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs > 0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "\n",
    "    return path_list, issame_list\n",
    "\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path+'.jpg'):\n",
    "        return path+'.jpg'\n",
    "    elif os.path.exists(path+'.png'):\n",
    "        return path+'.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)\n",
    "\n",
    "def read_pairs(pairs_filename):\n",
    "    pairs = []\n",
    "    with open(pairs_filename, 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "    return np.array(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read list of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1449011786' '0' '1095451819' '0' 'T']\n",
      " ['1448039576' '0' '1185423527' '0' 'F']\n",
      " ['1447550782' '0' '1462095693' '0' 'T']\n",
      " ['1458379276' '0' '1381475296' '0' 'F']\n",
      " ['1447550782' '0' '1230636915' '0' 'T']\n",
      " ['1447059340' '0' '1462029352' '0' 'F']]\n",
      "['1448039576' '0' '1217865434' '0' 'F']\n",
      "['/root/icon180k_image_160/1449011786/0.png', '/root/icon180k_image_160/1095451819/0.png', '/root/icon180k_image_160/1448039576/0.png', '/root/icon180k_image_160/1185423527/0.png', '/root/icon180k_image_160/1447550782/0.png', '/root/icon180k_image_160/1462095693/0.png', '/root/icon180k_image_160/1458379276/0.png', '/root/icon180k_image_160/1381475296/0.png']\n"
     ]
    }
   ],
   "source": [
    "# lfw_pairs='/root/facenet_davidsandberg/data/icon_valv2.txt'\n",
    "# lfw_dir='/root/icon180k_image_160/'\n",
    "\n",
    "# pairs = read_pairs(os.path.expanduser(lfw_pairs))\n",
    "# print(pairs[0:6])\n",
    "# print(pairs[-1])\n",
    "\n",
    "\n",
    "# # Get the paths for the corresponding images\n",
    "# lfw_paths, actual_issame = get_paths(os.path.expanduser(lfw_dir), pairs)\n",
    "# print(lfw_paths[0:8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read list of test set\n",
    "from jason file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read list of train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462898\n"
     ]
    }
   ],
   "source": [
    "input_dir='/root/icon81k_train_168_dup3_rescale'\n",
    "nrof_images_total = 0\n",
    "nrof_successfully_aligned = 0\n",
    "labels = []\n",
    "label_ = 0\n",
    "jpg_filenames = []\n",
    "for folder in os.listdir(input_dir):\n",
    "    nrof_images_total += 1\n",
    "    input_class_dir = os.path.join(input_dir, folder)\n",
    "    newClass=True\n",
    "    for filename in os.listdir(input_class_dir):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(input_class_dir, filename)\n",
    "            if newClass:\n",
    "                label_ += 1\n",
    "                newClass = False\n",
    "            labels.append(label_)\n",
    "            jpg_filenames.append(image_path)\n",
    "            \n",
    "\n",
    "#     if nrof_images_total >= 3:\n",
    "#         break\n",
    "# print(jpg_filenames)\n",
    "# print(labels)\n",
    "print(len(jpg_filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(input_class_dir))\n",
    "# print(len(os.listdir(input_class_dir)))\n",
    "# print(os.listdir('/root/aabb'))\n",
    "# print(len((os.listdir('/root/aabb'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read validation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create icon train and test for npair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 12 17:34:25 2019\n",
    "\n",
    "@author: Cheng-Hung\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "import fuel\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def preprocess(hwc_bgr_image, size):\n",
    "    hwc_rgb_image = cv2.cvtColor(hwc_bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    resized = cv2.resize(hwc_rgb_image, (size))\n",
    "    chw_image = np.transpose(resized, axes=(2, 0, 1))\n",
    "    return chw_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dataset_name = \"cars196\"\n",
    "    archive_basename = \"car_ims\"\n",
    "\n",
    "    fuel_root_path = \"./data\"\n",
    "#     fuel_root_path = \"./datasets/data\"\n",
    "    fuel_data_path = os.path.join(fuel_root_path, dataset_name)\n",
    "#     image_filepath = os.path.join(fuel_data_path, archive_basename + \".tar.gz\")\n",
    "#     label_filepath = os.path.join(fuel_data_path, \"cars_annos.mat\")\n",
    "\n",
    "#     # Extract car_ims.tgz if car_ims directory does not exist\n",
    "#     with tarfile.open(image_filepath, \"r\") as tf:\n",
    "#         jpg_filenames = [fn for fn in tf.getnames() if fn.endswith(\".jpg\")]\n",
    "#     jpg_filenames.sort()\n",
    "#     num_examples = len(jpg_filenames)  # ????\n",
    "#     if not os.path.exists(os.path.join(fuel_data_path, archive_basename)):\n",
    "#         subprocess.call([\"tar\", \"zxvf\", image_filepath.replace(\"\\\\\", \"/\"),\n",
    "#                          \"-C\", fuel_data_path.replace(\"\\\\\", \"/\"),\n",
    "#                          \"--force-local\"])\n",
    "# jpg_filenames = lfw_paths\n",
    "num_examples = len(jpg_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Extract class labels\n",
    "# #     cars_annos = loadmat(label_filepath)\n",
    "# #     annotations = cars_annos[\"annotations\"].ravel()\n",
    "# #     annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n",
    "#     annotations = range(num_examples)\n",
    "#     class_labels = []\n",
    "#     for annotation in annotations:\n",
    "# #         class_label = int(annotation[5])\n",
    "#         class_labels.append(annotation)\n",
    "\n",
    "# print(class_labels[0:4])\n",
    "class_labels=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # open hdf5 file\n",
    "    hdf5_filename = \"cars196.hdf5\"\n",
    "    hdf5_filepath = os.path.join(fuel_data_path, hdf5_filename)\n",
    "    hdf5 = h5py.File(hdf5_filepath, mode=\"w\")\n",
    "\n",
    "    # store images\n",
    "    image_size = (227, 227)\n",
    "    array_shape = (num_examples, 3) + image_size\n",
    "    ds_images = hdf5.create_dataset(\"images\", array_shape, dtype=np.uint8)\n",
    "    ds_images.dims[0].label = \"batch\"\n",
    "    ds_images.dims[1].label = \"channel\"\n",
    "    ds_images.dims[2].label = \"height\"\n",
    "    ds_images.dims[3].label = \"width\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./data/cars196/cars196.hdf5: 100%|██████████| 6000/6000 [00:08<00:00, 728.04it/s]\n"
     ]
    }
   ],
   "source": [
    "    # write images to the disk\n",
    "    for i, filename in tqdm(enumerate(jpg_filenames), total=num_examples,\n",
    "                            desc=hdf5_filepath):\n",
    "        raw_image = cv2.imread( filename,\n",
    "                               cv2.IMREAD_COLOR)  # BGR image\n",
    "        image = preprocess(raw_image, image_size)\n",
    "        ds_images[i] = image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # store the targets (class labels)\n",
    "    targets = np.array(class_labels, np.int32).reshape(num_examples, 1)\n",
    "    ds_targets = hdf5.create_dataset(\"targets\", data=targets)\n",
    "    ds_targets.dims[0].label = \"batch\"\n",
    "    ds_targets.dims[1].label = \"class_labels\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "    # specify the splits (labels 1~98 for train, 99~196 for test)\n",
    "#     test_head = class_labels.index(0)\n",
    "    test_head = len(class_labels) \n",
    "    print(test_head)\n",
    "    split_train, split_test = (0, test_head), (test_head, num_examples)\n",
    "    split_dict = dict(train=dict(images=split_train, targets=split_train),\n",
    "                      test=dict(images=split_test, targets=split_test))\n",
    "    hdf5.attrs[\"split\"] = H5PYDataset.create_split_array(split_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hdf5.flush()\n",
    "    hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 file>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load: test purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuel.datasets import H5PYDataset\n",
    "from fuel.utils import find_in_data_path\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.streams import DataStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/HDML/datasets/data/cars196/cars196.hdf5\"\n",
    "which_sets=['train', 'test']\n",
    "# aa=H5PYDataset(file_or_path=path, which_sets=which_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import IterationScheme, BatchSizeScheme, SequentialScheme\n",
    "\n",
    "from cars196_dataset import Cars196Dataset\n",
    "from cub200_2011_dataset import Cub200_2011Dataset\n",
    "from online_products_dataset import OnlineProductsDataset\n",
    "from random_fixed_size_crop_mod import RandomFixedSizeCrop\n",
    "class NPairLossScheme(BatchSizeScheme):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self._labels = np.array(labels).flatten()\n",
    "        self._label_encoder = LabelEncoder().fit(self._labels)\n",
    "        self._classes = self._label_encoder.classes_\n",
    "        self.num_classes = len(self._classes)\n",
    "        assert batch_size % 2 == 0, (\"batch_size must be even number.\")\n",
    "        assert batch_size <= self.num_classes * 2, (\n",
    "               \"batch_size must not exceed twice the number of classes\"\n",
    "               \"(i.e. set batch_size <= {}).\".format(self.num_classes * 2))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._class_to_indexes = []\n",
    "        for c in self._classes:\n",
    "            self._class_to_indexes.append(\n",
    "                np.argwhere(self._labels == c).ravel())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        anchor_indexes, positive_indexes = self._generate_indexes()\n",
    "        indexes = anchor_indexes + positive_indexes\n",
    "        return indexes\n",
    "\n",
    "    def _generate_indexes(self):\n",
    "        random_classes = np.random.choice(\n",
    "            self.num_classes, self.batch_size // 2, False)\n",
    "        anchor_indexes = []\n",
    "        positive_indexes = []\n",
    "        for c in random_classes:\n",
    "            a, p = np.random.choice(self._class_to_indexes[c], 2, False)\n",
    "            anchor_indexes.append(a)\n",
    "            positive_indexes.append(p)\n",
    "        return anchor_indexes, positive_indexes\n",
    "\n",
    "    def get_request_iterator(self):\n",
    "        return self\n",
    "\n",
    "class TripletLossScheme(BatchSizeScheme):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self._labels = np.array(labels).flatten()\n",
    "        self._label_encoder = LabelEncoder().fit(self._labels)\n",
    "        self._classes = self._label_encoder.classes_\n",
    "        self.num_classes = len(self._classes)\n",
    "        assert batch_size % 3 == 0, (\"batch_size must be 3*n.\")\n",
    "        assert batch_size <= self.num_classes * 3, (\n",
    "               \"batch_size must not exceed 3 times the number of classes\"\n",
    "               \"(i.e. set batch_size <= {}).\".format(self.num_classes * 3))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._class_to_indexes = []\n",
    "        for c in self._classes:\n",
    "            self._class_to_indexes.append(\n",
    "                np.argwhere(self._labels == c).ravel())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        anchor_indexes, positive_indexes, negative_indexes= self._generate_indexes()\n",
    "        indexes = anchor_indexes + positive_indexes + negative_indexes \n",
    "#        print indexes \n",
    "        return indexes\n",
    "\n",
    "    def _generate_indexes(self):\n",
    "        random_classes = random.sample(\n",
    "            list(range(self.num_classes)), self.batch_size // 3 * 2)\n",
    "        anchor_indexes = []\n",
    "        positive_indexes = []\n",
    "        negative_indexes = []\n",
    "        for i in range(self.batch_size // 3):\n",
    "            a, p = random.sample(list(self._class_to_indexes[random_classes[i]]), 2)\n",
    "            anchor_indexes.append(a)\n",
    "            positive_indexes.append(p)\n",
    "#            while \n",
    "#            c2 = np.random.choice(self.num_classes, 1)\n",
    "#            while c2[0]==c:\n",
    "#                c2 = np.random.choice(self.num_classes, 1)\n",
    "##            n = random.sample(list(self._class_to_indexes[c2[0]]), 1)\n",
    "#            print c2, n\n",
    "            n = random.sample(list(\n",
    "                    self._class_to_indexes[random_classes[i+self.batch_size // 3]]), 1)\n",
    "            negative_indexes.append(n[0])\n",
    "#        print anchor_indexes, positive_indexes, negative_indexes\n",
    "#        positive_indexe\n",
    "        return anchor_indexes, positive_indexes, negative_indexes\n",
    "    def get_request_iterator(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class ContrastiveLossScheme(BatchSizeScheme):\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self._labels = np.array(labels).flatten()\n",
    "        self._label_encoder = LabelEncoder().fit(self._labels)\n",
    "        self._classes = self._label_encoder.classes_\n",
    "        self.num_classes = len(self._classes)\n",
    "        assert batch_size % 4 == 0, (\"batch_size must be 2*n.\")\n",
    "        assert batch_size <= self.num_classes * 2, (\n",
    "               \"batch_size must not exceed twice times the number of classes\"\n",
    "               \"(i.e. set batch_size <= {}).\".format(self.num_classes * 2))\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self._class_to_indexes = []\n",
    "        for c in self._classes:\n",
    "            self._class_to_indexes.append(\n",
    "                np.argwhere(self._labels == c).ravel())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        pos1,pos2,neg1,neg2= self._generate_indexes()\n",
    "        indexes = pos1+pos2+neg1+neg2\n",
    "#        print indexes \n",
    "        return indexes\n",
    "\n",
    "    def _generate_indexes(self):\n",
    "        #indexes = random.sample(range(len(self._labels)),self.batch_size)\n",
    "        random_classes = random.sample(\n",
    "            list(range(self.num_classes)), self.num_classes)\n",
    "        pos1 = []\n",
    "        pos2 = []\n",
    "        neg1 = []\n",
    "        neg2 = []\n",
    "        for i in range(self.batch_size // 4):\n",
    "            a, p = random.sample(list(self._class_to_indexes[random_classes[i]]), 2)\n",
    "            pos1.append(a)\n",
    "            pos2.append(p)\n",
    "        for i in range((self.batch_size//4),(self.batch_size//4)*2):\n",
    "            n1 = random.sample(list(\n",
    "                    self._class_to_indexes[random_classes[i]]), 1)\n",
    "            n2 = random.sample(list(\n",
    "                    self._class_to_indexes[random_classes[i+self.batch_size // 4]]), 1)\n",
    "            neg1.append(n1)\n",
    "            neg2.append(n2)\n",
    "            \n",
    "        '''\n",
    "        matches = []\n",
    "        for i in range(self.batch_size // 2):\n",
    "            if anchor_indexes[i] == positive_indexes[i]:\n",
    "                matches += [1]\n",
    "            else:\n",
    "                matches += [0]\n",
    "        anchor_indexes = np.array(anchor_indexes)\n",
    "        positive_indexes = np.array(positive_indexes)\n",
    "        matches = np.array(matches)    \n",
    "        '''\n",
    "        return pos1,pos2,neg1,neg2\n",
    "                \n",
    "#        print anchor_indexes, positive_indexes, negative_indexes\n",
    "#        positive_indexe\n",
    "    def get_request_iterator(self):\n",
    "        return self\n",
    "\n",
    "class EpochwiseShuffledInfiniteScheme(BatchSizeScheme):\n",
    "    def __init__(self, indexes, batch_size):\n",
    "        if not isinstance(indexes, collections.Iterable):\n",
    "            indexes = range(indexes)\n",
    "        if batch_size > len(indexes):\n",
    "            raise ValueError('batch_size must not be larger than the indexes.')\n",
    "        if len(indexes) != len(np.unique(indexes)):\n",
    "            raise ValueError('Items in indexes must be unique.')\n",
    "        self._original_indexes = np.array(indexes, dtype=np.int).flatten()\n",
    "        self.batch_size = batch_size\n",
    "        self._shuffled_indexes = np.array([], dtype=np.int)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # if remaining indexes are shorter than batch_size then new shuffled\n",
    "        # indexes are appended to the remains.\n",
    "        num_remains = len(self._shuffled_indexes)\n",
    "        if num_remains < batch_size:\n",
    "            num_overrun = batch_size - num_remains\n",
    "            new_shuffled_indexes = self._original_indexes.copy()\n",
    "\n",
    "            # ensure the batch of indexes from the joint part does not contain\n",
    "            # duplicate index.\n",
    "            np.random.shuffle(new_shuffled_indexes)\n",
    "            overrun = new_shuffled_indexes[:num_overrun]\n",
    "            next_indexes = np.concatenate((self._shuffled_indexes, overrun))\n",
    "            while len(next_indexes) != len(np.unique(next_indexes)):\n",
    "                np.random.shuffle(new_shuffled_indexes)\n",
    "                overrun = new_shuffled_indexes[:num_overrun]\n",
    "                next_indexes = np.concatenate(\n",
    "                    (self._shuffled_indexes, overrun))\n",
    "            self._shuffled_indexes = np.concatenate(\n",
    "                (self._shuffled_indexes, new_shuffled_indexes))\n",
    "        next_indexes = self._shuffled_indexes[:batch_size]\n",
    "        self._shuffled_indexes = self._shuffled_indexes[batch_size:]\n",
    "        return next_indexes.tolist()\n",
    "\n",
    "    def get_request_iterator(self):\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cars196Dataset(H5PYDataset):\n",
    "\n",
    "    _filename = 'cars196/cars196.hdf5'\n",
    "\n",
    "    def __init__(self, which_sets, **kwargs):\n",
    "        try:\n",
    "#            path = find_in_data_path(self._filename)\n",
    "            path = \"/root/HDML/datasets/data/cars196/cars196.hdf5\"\n",
    "        except IOError as e:\n",
    "            msg = str(e) + (\"\"\".\n",
    "         You need to download the dataset and convert it to hdf5 before.\"\"\")\n",
    "            raise IOError(msg)\n",
    "        super(Cars196Dataset, self).__init__(\n",
    "            file_or_path=path, which_sets=which_sets, **kwargs)\n",
    "\n",
    "\n",
    "def load_as_ndarray(which_sets=['train', 'test']):\n",
    "    datasets = []\n",
    "    for split in which_sets:\n",
    "        data = Cars196Dataset([split], load_in_memory=True).data_sources\n",
    "        datasets.append(data)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = Cars196Dataset(['train'])\n",
    "\n",
    "    st = DataStream(\n",
    "        dataset, iteration_scheme=SequentialScheme(dataset.num_examples, 1))\n",
    "    it = st.get_epoch_iterator()\n",
    "    it.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_class = Cars196Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "    load_in_memory=False\n",
    "    dataset_train = dataset_class(['train'], load_in_memory=load_in_memory)\n",
    "    dataset_test = dataset_class(['test'], load_in_memory=load_in_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "        labels = dataset_class(\n",
    "            ['train'], sources=['targets'], load_in_memory=True).data_sources\n",
    "        scheme = NPairLossScheme(labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0],\n",
       "        [   1],\n",
       "        [   2],\n",
       "        ...,\n",
       "        [5996],\n",
       "        [5997],\n",
       "        [5998]], dtype=int32),)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5999"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(labels[0].shape)\n",
    "print(labels[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageout = dataset_class(\n",
    "            ['train'], sources=['images',], load_in_memory=True).data_sources\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 3, 256, 256)\n",
      "(3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(imageout[0].shape)\n",
    "print(imageout[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(imageout[0][0:4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 128,\n",
       " 256,\n",
       " 384,\n",
       " 512,\n",
       " 640,\n",
       " 768,\n",
       " 896,\n",
       " 1024,\n",
       " 1152,\n",
       " 1280,\n",
       " 1408,\n",
       " 1536,\n",
       " 1664,\n",
       " 1792,\n",
       " 1920,\n",
       " 2048,\n",
       " 2176,\n",
       " 2304,\n",
       " 2432,\n",
       " 2560,\n",
       " 2688,\n",
       " 2816,\n",
       " 2944,\n",
       " 3072,\n",
       " 3200,\n",
       " 3328,\n",
       " 3456,\n",
       " 3584,\n",
       " 3712,\n",
       " 3840,\n",
       " 3968,\n",
       " 4096,\n",
       " 4224,\n",
       " 4352,\n",
       " 4480,\n",
       " 4608,\n",
       " 4736,\n",
       " 4864,\n",
       " 4992,\n",
       " 5120,\n",
       " 5248,\n",
       " 5376,\n",
       " 5504,\n",
       " 5632,\n",
       " 5760,\n",
       " 5888]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0,len(labels[0]),128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./data/cars196/cars196.hdf5:   0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 160, 3)\n",
      "(3, 227, 227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    # write images to the disk\n",
    "    for i, filename in tqdm(enumerate(jpg_filenames), total=num_examples,\n",
    "                            desc=hdf5_filepath):\n",
    "        raw_image = cv2.imread( filename,\n",
    "                               cv2.IMREAD_COLOR)  # BGR image\n",
    "#         plt.imshow(raw_image)\n",
    "        \n",
    "        image = preprocess(raw_image, image_size)\n",
    "#         plt.imshow(image)\n",
    "        print(raw_image.shape)\n",
    "        print(image.shape)\n",
    "\n",
    "        break\n",
    "#         ds_images[i] = image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(imageout[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch_data = np.transpose(imageout[0][:, [2,1,0], :, :], (0, 2, 3, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 256, 256, 3)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_batch_data.shape)\n",
    "print(x_batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
